LoRA Training Dataset & Usage Guide

This repository provides the training file used to fine-tune six 7B instruction-tuned Large Language Models (LLMs) using LoRA (Low-Rank Adaptation).
It also demonstrates how to load and use the trained LoRA adapters from Hugging Face in Python.

ğŸ“Œ Models Trained with This Dataset

The same training file was used to fine-tune the following base models:

google/gemma-7b-it

tiiuae/falcon-7b-instruct

meta-llama/Llama-2-7b-chat-hf

Qwen/Qwen2.5-7B-Instruct

mistralai/Mistral-7B-Instruct-v0.2

ibm-granite/granite-7b-instruct

Each model was trained independently using LoRA to reduce compute and memory requirements while preserving instruction-following performance.

ğŸ“‚ Training File Description

Format: Instruction-based text (instruction / input / output)

Purpose: Fine-tuning instruction-following behavior

Method: Parameter-efficient fine-tuning using LoRA

Compatibility: Works with Hugging Face transformers + peft

This dataset can be reused to train additional LoRA adapters on other compatible 7B models.

ğŸ¤— Available LoRA Adapters on Hugging Face

The trained LoRA adapters are publicly available:

KraveTech/Falcon7B_LoRA

KraveTech/Mistral7B_LoRA

KraveTech/Gemma7B_LoRA

KraveTech/Llama7B_LoRA

KraveTech/Qwen7B_LoRA

KraveTech/Granite7B_LoRA

ğŸš€ How to Use a LoRA Adapter in Python

Below is a standard example for loading a base model and applying a LoRA adapter using Hugging Face.

ğŸ”§ Install Dependencies
pip install torch transformers peft accelerate

ğŸ§  Example: Load Base Model + LoRA Adapter
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Choose base model and LoRA adapter
base_model_name = "tiiuae/falcon-7b-instruct"
lora_adapter_name = "KraveTech/Falcon7B_LoRA"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model_name)

# Load base model
model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Load LoRA adapter
model = PeftModel.from_pretrained(model, lora_adapter_name)
model.eval()

# Run inference
prompt = "Explain what LoRA fine-tuning is in simple terms."

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        do_sample=True,
        temperature=0.7
    )

print(tokenizer.decode(outputs[0], skip_special_tokens=True))

ğŸ” Switching to Another LoRA Adapter

To use a different LoRA model, simply change:

base_model_name = "mistralai/Mistral-7B-Instruct-v0.2"
lora_adapter_name = "KraveTech/Mistral7B_LoRA"


âš ï¸ Important:
The LoRA adapter must match the base model architecture it was trained on.

ğŸ“Š Benefits of LoRA Training

âœ… Reduced GPU memory usage

âœ… Faster fine-tuning

âœ… No modification to original model weights

âœ… Easy adapter swapping

ğŸ“œ License & Usage

Base models are subject to their original licenses.

LoRA adapters are released for research and development purposes.

Please review Hugging Face model licenses before commercial use.

ğŸ™Œ Acknowledgements

Hugging Face ğŸ¤—

PEFT library

Open-source LLM community
